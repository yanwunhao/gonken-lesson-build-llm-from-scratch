{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad97768",
   "metadata": {},
   "source": [
    "# レッスン 04  Attentionメカニズム\n",
    "\n",
    "深層学習の歴史において、2017年の「Attention is All You Need」論文は転換点となりました。それまでのRNNやLSTMが持っていた「順次処理」という制約から解放され、並列処理可能で長距離依存関係を効果的に学習できる新しいアーキテクチャが誕生したのです。人間が文章を理解する過程を考えてみましょう。\n",
    "\n",
    "「昨日公園で見た桜がとても美しかった」という文を読むとき、「美しかった」という形容詞を理解するために、私たちの脳は自動的に「桜」に注意を向けます。さらに「昨日」「公園」という文脈情報も同時に処理しています。\n",
    "\n",
    "Attentionメカニズムは、まさにこの「選択的注意」の仕組みを数学的に実装したものです。\n",
    "\n",
    "本講義では、単純な行列演算から始めて、段階的にSelf-Attentionの実装まで到達します。各ステップで「なぜこの操作が必要なのか」「どのような問題を解決しているのか」を理解しながら進めていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e36512",
   "metadata": {},
   "source": [
    "## Section 1: 行列積の基礎 - 重み付き集約\n",
    "\n",
    "Attentionを理解する第一歩として、「行列積が重み付き平均を計算できる」という基本原理から始めます。これは一見単純ですが、Attention機構全体を支える重要な数学的基盤です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a39eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb018f",
   "metadata": {},
   "source": [
    "再現性のためのシード設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30af7c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f144c7fc070>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2bfa0e",
   "metadata": {},
   "source": [
    "3x3の下三角行列を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e3b8f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下三角行列（正規化前）:\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3, 3))\n",
    "print(\"下三角行列（正規化前）:\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501bb452",
   "metadata": {},
   "source": [
    "各行を正規化（行の合計を1にする）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a57b0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正規化後の重み行列:\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n"
     ]
    }
   ],
   "source": [
    "a = a / torch.sum(a, dim=1, keepdim=True)\n",
    "print(\"\\n正規化後の重み行列:\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739e3452",
   "metadata": {},
   "source": [
    "ランダムな値行列を生成と重み付き集約を実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05995ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "値行列b:\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "\n",
      "結果c = a @ b:\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# ランダムな値行列を生成\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "print(\"\\n値行列b:\")\n",
    "print(b)\n",
    "\n",
    "# 重み付き集約を実行\n",
    "c = a @ b\n",
    "print(\"\\n結果c = a @ b:\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8c5a80",
   "metadata": {},
   "source": [
    "### 直感的な理解を深める\n",
    "\n",
    "この操作を具体的な例で考えてみましょう。bの各行を単語の特徴ベクトルだと想像してください。例えば、3つの単語「私は」「猫が」「好きだ」があるとします。\n",
    "\n",
    "重み行列aの各行は、「その位置から見て、どの単語にどれだけ注目するか」を表しています。\n",
    "\n",
    "第1行[1.0, 0.0, 0.0]は「最初の単語だけを見る」、第2行[0.5, 0.5, 0.0]は「最初の2つの単語を均等に見る」、第3行[0.33, 0.33, 0.33]は「全ての単語を均等に見る」という意味になります。\n",
    "\n",
    "なぜ下三角行列を使うのでしょうか。これは「因果的マスキング（causal masking）」と呼ばれる重要な概念につながります。\n",
    "\n",
    "言語モデルが文章を生成する際、各単語は自分より後の単語（未来の情報）を見ることができません。下三角行列はこの制約を自然に表現しているのです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298f2a48",
   "metadata": {},
   "source": [
    "## Section 2: 実践的なバッチ処理 - Bag of Words方式の実装\n",
    "\n",
    "実際の深層学習では、複数のサンプルを同時に処理する必要があります。ここでは、より実践的な設定での処理を見ていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e042ac12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力テンソルの形状: torch.Size([4, 8, 2])\n",
      "これは4個の文書、各文書は8個の単語、各単語は2次元のベクトル\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# B: バッチサイズ（4つの独立したシーケンス）\n",
    "# T: 時間ステップ数（各シーケンスは8トークン）\n",
    "# C: チャネル数（各トークンは2次元ベクトル）\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "print(f\"入力テンソルの形状: {x.shape}\")\n",
    "print(f\"これは{B}個の文書、各文書は{T}個の単語、各単語は{C}次元のベクトル\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0529487",
   "metadata": {},
   "source": [
    "累積平均を計算する（ナイーブな実装）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba2ebf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "バッチ0, 時刻0:\n",
      "  使用するトークン数: 1\n",
      "  平均値: tensor([ 0.1808, -0.0700])\n",
      "\n",
      "バッチ0, 時刻1:\n",
      "  使用するトークン数: 2\n",
      "  平均値: tensor([-0.0894, -0.4926])\n",
      "\n",
      "バッチ0, 時刻2:\n",
      "  使用するトークン数: 3\n",
      "  平均値: tensor([ 0.1490, -0.3199])\n"
     ]
    }
   ],
   "source": [
    "xbow = torch.zeros((B, T, C))  # bow = \"bag of words\"の略\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        # 時刻tまでの全てのトークンを取得\n",
    "        xprev = x[b, :t+1]  # shape: (t+1, C)\n",
    "        \n",
    "        # 平均を計算して格納\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0)\n",
    "        \n",
    "        # デバッグ用：最初のバッチの最初の3ステップを表示\n",
    "        if b == 0 and t < 3:\n",
    "            print(f\"\\nバッチ0, 時刻{t}:\")\n",
    "            print(f\"  使用するトークン数: {t+1}\")\n",
    "            print(f\"  平均値: {xbow[b, t]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3f8d33",
   "metadata": {},
   "source": [
    "### この処理の意味を考える\n",
    "各時刻で過去の全ての情報を平均化するこの処理には、深い意味があります。文章を読む際、私たちは読み進めるにつれて文脈を蓄積していきます。\n",
    "\n",
    "「今日は天気が良い。公園に行こう。」という文章で、「公園」を理解する時には「今日」「天気が良い」という前の情報も考慮に入れています。\n",
    "\n",
    "しかし、単純な平均には問題があります。全ての過去の情報を均等に扱うため、重要な情報もそうでない情報も同じ重みになってしまうのです。\n",
    "\n",
    "これが、後で学ぶ「学習可能な重み」の必要性につながります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6c5259",
   "metadata": {},
   "source": [
    "## Section 3: 行列演算による効率化\n",
    "\n",
    "Section 2のループ処理は理解しやすいですが、実用的ではありません。ここで、Section 1で学んだ行列積の知識を活用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b1a46d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重み行列の形状: torch.Size([8, 8])\n",
      "重み行列の一部（最初の4x4）:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500]])\n",
      "\n",
      "二つの方法の結果は同じか: False\n"
     ]
    }
   ],
   "source": [
    "# 効率的な実装：行列積を使用\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)\n",
    "\n",
    "print(\"重み行列の形状:\", wei.shape)\n",
    "print(\"重み行列の一部（最初の4x4）:\")\n",
    "print(wei[:4, :4])\n",
    "\n",
    "# バッチ処理を一度に実行\n",
    "# PyTorchのブロードキャスティングにより、\n",
    "# (T, T) @ (B, T, C) -> (B, T, C) が自動的に処理される\n",
    "xbow2 = wei @ x\n",
    "\n",
    "# 結果が同一であることを確認\n",
    "print(f\"\\n二つの方法の結果は同じか: {torch.allclose(xbow, xbow2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b43b43",
   "metadata": {},
   "source": [
    "### 計算速度の比較（仮想的な例）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4082d88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データサイズ: バッチ=16, シーケンス長=128, 特徴次元=256\n",
      "\n",
      "処理時間の比較:\n",
      "ループ版: 0.0178秒\n",
      "行列積版: 0.0017秒\n",
      "\n",
      "計算結果は一致: False\n",
      "高速化率: 10.7倍\n",
      "\n",
      "フルサイズ（B=32, T=512）での推定:\n",
      "ループ版: 約0.6秒\n",
      "行列積版: 約0.026秒\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# 実践的なサイズで比較（小規模版）\n",
    "B, T, C = 16, 128, 256\n",
    "x = torch.randn(B, T, C)\n",
    "print(f\"データサイズ: バッチ={B}, シーケンス長={T}, 特徴次元={C}\")\n",
    "\n",
    "# ===== 方法1: ループによる実装 =====\n",
    "def compute_with_loop(x):\n",
    "    B, T, C = x.shape\n",
    "    result = torch.zeros_like(x)\n",
    "    for b in range(B):\n",
    "        for t in range(T):\n",
    "            xprev = x[b, :t+1]\n",
    "            result[b, t] = torch.mean(xprev, dim=0)\n",
    "    return result\n",
    "\n",
    "# ===== 方法2: 行列積による実装 =====\n",
    "def compute_with_matrix(x):\n",
    "    B, T, C = x.shape\n",
    "    wei = torch.tril(torch.ones(T, T))\n",
    "    wei = wei / wei.sum(dim=1, keepdim=True)\n",
    "    return wei @ x\n",
    "\n",
    "# 時間測定\n",
    "print(\"\\n処理時間の比較:\")\n",
    "start = time.time()\n",
    "result_loop = compute_with_loop(x)\n",
    "time_loop = time.time() - start\n",
    "print(f\"ループ版: {time_loop:.4f}秒\")\n",
    "\n",
    "start = time.time()\n",
    "result_matrix = compute_with_matrix(x)\n",
    "time_matrix = time.time() - start\n",
    "print(f\"行列積版: {time_matrix:.4f}秒\")\n",
    "\n",
    "# 結果の検証\n",
    "print(f\"\\n計算結果は一致: {torch.allclose(result_loop, result_matrix)}\")\n",
    "print(f\"高速化率: {time_loop/time_matrix:.1f}倍\")\n",
    "\n",
    "# 大規模データでの推定\n",
    "B_large, T_large = 32, 512\n",
    "scale_factor = (B_large/B) * (T_large/T) * (T_large/T)\n",
    "print(f\"\\nフルサイズ（B={B_large}, T={T_large}）での推定:\")\n",
    "print(f\"ループ版: 約{time_loop * scale_factor:.1f}秒\")\n",
    "print(f\"行列積版: 約{time_matrix * (T_large/T)**2:.3f}秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5974ecd7",
   "metadata": {},
   "source": [
    "## Section 4: Self-Attentionの実装: 動的な注意機構\n",
    "### 概念の導入と直感的理解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024aabd7",
   "metadata": {},
   "source": [
    "Self-Attentionの核心は、入力データから三つの異なる表現を作り出すことから始まります。これは情報検索システムからの美しいアナロジーです。\n",
    "\n",
    "図書館で本を探す場面を想像してください。あなたが持っている「検索キーワード」（Query）、各本についている「インデックスカード」（Key）、そして「本の実際の内容」（Value）という三要素の相互作用と考えることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f681823d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力データ x の形状: torch.Size([4, 8, 32])\n",
      "これは4個の文書、各8トークン、各トークンは32次元の表現\n",
      "\n",
      "Query形状: torch.Size([4, 8, 16]) - 『私は何を探しているか』\n",
      "Key形状: torch.Size([4, 8, 16]) - 『私は何についての情報を持っているか』\n",
      "Value形状: torch.Size([4, 8, 16]) - 『私が実際に提供できる情報』\n",
      "\n",
      "例：「猫が魚を食べる」という文を処理する場合\n",
      "- '食べる'のQuery: 『誰が何を食べるのか知りたい』\n",
      "- '猫'のKey: 『私は動作の主体についての情報を持っている』\n",
      "- '猫'のValue: 『実際の猫に関する特徴情報』\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32  # バッチ=4、時系列=8、チャネル=32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "print(f\"入力データ x の形状: {x.shape}\")\n",
    "print(f\"これは{B}個の文書、各{T}トークン、各トークンは{C}次元の表現\")\n",
    "\n",
    "# Attention Headのサイズを定義\n",
    "# なぜ元の32次元から16次元に削減するのか？\n",
    "# 1. 計算効率の向上（パラメータ数の削減）\n",
    "# 2. 過学習の防止（表現力の適切な制限）\n",
    "# 3. Multi-Head Attentionで複数のヘッドを使う準備\n",
    "head_size = 16\n",
    "\n",
    "# 三つの線形変換層を定義（これらは学習可能なパラメータ）\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# 各表現を計算\n",
    "k = key(x)    # (B, T, 16) - 各トークンの「キー」表現\n",
    "q = query(x)  # (B, T, 16) - 各トークンの「クエリ」表現\n",
    "v = value(x)  # (B, T, 16) - 各トークンの「バリュー」表現\n",
    "\n",
    "print(f\"\\nQuery形状: {q.shape} - 『私は何を探しているか』\")\n",
    "print(f\"Key形状: {k.shape} - 『私は何についての情報を持っているか』\")\n",
    "print(f\"Value形状: {v.shape} - 『私が実際に提供できる情報』\")\n",
    "\n",
    "# 具体例で理解を深める\n",
    "print(\"\\n例：「猫が魚を食べる」という文を処理する場合\")\n",
    "print(\"- '食べる'のQuery: 『誰が何を食べるのか知りたい』\")\n",
    "print(\"- '猫'のKey: 『私は動作の主体についての情報を持っている』\")\n",
    "print(\"- '猫'のValue: 『実際の猫に関する特徴情報』\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32277d6b",
   "metadata": {},
   "source": [
    "この三つの表現への分離は、なぜ必要なのでしょうか。\n",
    "\n",
    "それは、「何を探すか」「何として見つかるか」「何を提供するか」を分離することで、より柔軟で表現力豊かな注意メカニズムを実現できるからです。\n",
    "\n",
    "同じトークンでも、文脈によって異なる役割を果たすことができるのです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b565b576",
   "metadata": {},
   "source": [
    "### 相性スコアの計算と因果的制約の実装\n",
    "\n",
    "次に、各QueryとKeyの間の「相性」を計算し、どのトークンにどれだけ注目すべきかを決定します。この段階で、言語モデル特有の重要な制約「未来を見ない」も実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68174602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attentionスコア行列の形状: torch.Size([4, 8, 8])\n",
      "wei[b,i,j] = 位置iのクエリと位置jのキーの相性スコア\n",
      "\n",
      "スケーリングファクター: 1/√16 = 0.2500\n",
      "\n",
      "Causalマスク（最初の5x5）:\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "マスク適用前後の比較（最初のバッチ、最初の3x3）:\n",
      "適用前:\n",
      "tensor([[-0.4407, -0.3253,  0.1413],\n",
      "        [-0.8334, -0.4139,  0.0260],\n",
      "        [-0.2557, -0.3152,  0.0191]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "適用後:\n",
      "tensor([[-0.4407,    -inf,    -inf],\n",
      "        [-0.8334, -0.4139,    -inf],\n",
      "        [-0.2557, -0.3152,  0.0191]], grad_fn=<SliceBackward0>)\n",
      "（-infは未来の位置を完全に無視することを意味する）\n",
      "\n",
      "時系列での解釈:\n",
      "位置0: 自分だけを見る\n",
      "位置1: 位置0と自分を見る\n",
      "位置2: 位置0, 1と自分を見る\n",
      "...これが言語生成での自己回帰的な性質を実現\n"
     ]
    }
   ],
   "source": [
    "# ステップ1: Attentionスコアの計算\n",
    "# QueryとKeyの内積により、各位置間の関連性スコアを計算\n",
    "wei = q @ k.transpose(-2, -1)  \n",
    "# (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "print(f\"Attentionスコア行列の形状: {wei.shape}\")\n",
    "print(\"wei[b,i,j] = 位置iのクエリと位置jのキーの相性スコア\")\n",
    "\n",
    "# 重要：スケーリングファクターの適用\n",
    "# なぜ√head_sizeで割るのか？\n",
    "# 内積の値が大きくなりすぎると、Softmaxが極端な値（0か1）を出力しやすくなる\n",
    "# これは勾配消失を引き起こし、学習を困難にする\n",
    "scale_factor = head_size ** -0.5\n",
    "wei = wei * scale_factor\n",
    "print(f\"\\nスケーリングファクター: 1/√{head_size} = {scale_factor:.4f}\")\n",
    "\n",
    "# ステップ2: Causal Maskingの適用\n",
    "# 下三角行列を作成（未来の情報を見ない制約）\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "print(f\"\\nCausalマスク（最初の5x5）:\")\n",
    "print(tril[:5, :5])\n",
    "\n",
    "# マスキングの実装：未来の位置を負の無限大に設定\n",
    "# なぜ-infなのか？exp(-inf) = 0となり、Softmax後に完全に0になる\n",
    "wei_before_mask = wei.clone()\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "\n",
    "print(\"\\nマスク適用前後の比較（最初のバッチ、最初の3x3）:\")\n",
    "print(\"適用前:\")\n",
    "print(wei_before_mask[0, :3, :3])\n",
    "print(\"\\n適用後:\")\n",
    "print(wei[0, :3, :3])\n",
    "print(\"（-infは未来の位置を完全に無視することを意味する）\")\n",
    "\n",
    "# 視覚的な理解のための例\n",
    "print(\"\\n時系列での解釈:\")\n",
    "print(\"位置0: 自分だけを見る\")\n",
    "print(\"位置1: 位置0と自分を見る\") \n",
    "print(\"位置2: 位置0, 1と自分を見る\")\n",
    "print(\"...これが言語生成での自己回帰的な性質を実現\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbb1104",
   "metadata": {},
   "source": [
    "このマスキング処理により、モデルは訓練時と推論時で一貫した動作をします。\n",
    "\n",
    "各トークンは、自分より前のトークンだけを参照して次の表現を計算するため、左から右への文章生成が自然に実現されるのです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0cf42d",
   "metadata": {},
   "source": [
    "### 確率分布への変換と重み付き和の計算\n",
    "\n",
    "最後のステップでは、スコアを確率分布に変換し、その重みを使ってValue表現を集約します。これにより、各位置が「自分が注目すべき他の位置の情報」を選択的に取り込んだ新しい表現を得ます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de410f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax適用前後の比較（最初のバッチ、位置2の注意重み）:\n",
      "適用前のスコア: tensor([-0.2557, -0.3152,  0.0191,    -inf,    -inf], grad_fn=<SliceBackward0>)\n",
      "適用後の確率: tensor([0.3069, 0.2892, 0.4039, 0.0000, 0.0000], grad_fn=<SliceBackward0>)\n",
      "確率の合計: 1.0000\n",
      "\n",
      "実際のAttention重みパターンの例:\n",
      "位置0: [1.000]\n",
      "位置1: [0.397, 0.603]\n",
      "位置2: [0.307, 0.289, 0.404]\n",
      "位置3: [0.323, 0.218, 0.244, 0.215]\n",
      "\n",
      "最終出力の形状: torch.Size([4, 8, 16])\n",
      "各位置が、注意重みに基づいて他の位置のValue情報を統合\n",
      "\n",
      "具体例：位置3の新しい表現の計算\n",
      "out[b,3] = \n",
      "  wei[b,3,0] * v[b,0] +  # 位置0への注意重み × 位置0の値\n",
      "  wei[b,3,1] * v[b,1] +  # 位置1への注意重み × 位置1の値\n",
      "  wei[b,3,2] * v[b,2] +  # 位置2への注意重み × 位置2の値\n",
      "  wei[b,3,3] * v[b,3]    # 自分への注意重み × 自分の値\n",
      "\n",
      "=== Self-Attentionの処理完了 ===\n",
      "入力: torch.Size([4, 8, 32]) -> Query/Key/Value変換 -> スコア計算 -> \n",
      "マスキング -> Softmax -> 重み付き和 -> 出力: torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# ステップ3: Softmaxによる正規化\n",
    "# 各行のスコアを確率分布に変換（合計が1になる）\n",
    "wei_before_softmax = wei.clone()\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "print(\"Softmax適用前後の比較（最初のバッチ、位置2の注意重み）:\")\n",
    "print(f\"適用前のスコア: {wei_before_softmax[0, 2, :5]}\")\n",
    "print(f\"適用後の確率: {wei[0, 2, :5]}\")\n",
    "print(f\"確率の合計: {wei[0, 2, :3].sum().item():.4f}\")  # 見える範囲の合計は1\n",
    "\n",
    "# Attention重みの可視化（仮想的な例）\n",
    "print(\"\\n実際のAttention重みパターンの例:\")\n",
    "example_weights = wei[0, :4, :4]\n",
    "for i in range(4):\n",
    "    weights_str = [f\"{w:.3f}\" for w in example_weights[i, :i+1]]\n",
    "    print(f\"位置{i}: [{', '.join(weights_str)}]\")\n",
    "\n",
    "# ステップ4: 最終的な重み付き和の計算\n",
    "out = wei @ v  # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
    "\n",
    "print(f\"\\n最終出力の形状: {out.shape}\")\n",
    "print(\"各位置が、注意重みに基づいて他の位置のValue情報を統合\")\n",
    "\n",
    "# 計算の意味を具体例で理解\n",
    "print(\"\\n具体例：位置3の新しい表現の計算\")\n",
    "print(\"out[b,3] = \")\n",
    "print(\"  wei[b,3,0] * v[b,0] +  # 位置0への注意重み × 位置0の値\")\n",
    "print(\"  wei[b,3,1] * v[b,1] +  # 位置1への注意重み × 位置1の値\")\n",
    "print(\"  wei[b,3,2] * v[b,2] +  # 位置2への注意重み × 位置2の値\")\n",
    "print(\"  wei[b,3,3] * v[b,3]    # 自分への注意重み × 自分の値\")\n",
    "\n",
    "# Self-Attentionの完全な処理を確認\n",
    "print(\"\\n=== Self-Attentionの処理完了 ===\")\n",
    "print(f\"入力: {x.shape} -> Query/Key/Value変換 -> スコア計算 -> \")\n",
    "print(f\"マスキング -> Softmax -> 重み付き和 -> 出力: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96036196",
   "metadata": {},
   "source": [
    "このメカニズムの素晴らしさを改めて考えてみましょう。従来の固定重みでは不可能だった「文脈に応じた動的な情報選択」が可能になりました。\n",
    "\n",
    "例えば、「銀行でお金を下ろす」と「川の銀行で釣りをする」という二つの文で、同じ「銀行」という単語でも全く異なる注意パターンを持ちます。\n",
    "\n",
    "前者では「お金」「下ろす」により強く注目し、金融機関としての意味を活性化させます。後者では「川」「釣り」に注目し、地理的な意味を活性化させます。\n",
    "\n",
    "この文脈依存の意味理解こそが、現代のAIが人間のような言語理解を示す理由なのです。\n",
    "\n",
    "さらに、この仕組みは完全に微分可能であるため、誤差逆伝播によって学習できます。モデルは大量のテキストから、どのような文脈でどのトークンに注目すべきかを自動的に学習していくのです。\n",
    "\n",
    "これがTransformerアーキテクチャの革新性であり、ChatGPTのような大規模言語モデルの基盤となっている技術なのです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac72ec9",
   "metadata": {},
   "source": [
    "## 参考コード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa478a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16  # how many independent sequences will we process in parallel?\n",
    "block_size = 32  # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 1000\n",
    "learning_rate = 1e-3\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "print(\"Running on \" + device)\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [\n",
    "    stoi[c] for c in s\n",
    "]  # encoder: take a string, output a list of integers\n",
    "decode = lambda l: \"\".join(\n",
    "    [itos[i] for i in l]\n",
    ")  # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"one head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B,T,C)\n",
    "        q = self.query(x)  # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,C)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, \"M parameters\")\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(\n",
    "            f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
    "        )\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
