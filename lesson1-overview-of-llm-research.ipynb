{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2319c26f",
   "metadata": {},
   "source": [
    "# レッスン 01  LLMの研究概要\n",
    "\n",
    "このコースは、スタンフォード大学の公開講座であるCS336とCS224N、そしてAndrej KarpathyのYouTubeブログのコンテンツをまとめたものです。\n",
    "お時間があれば、YouTubeでオリジナル動画、**特にCS336**の動画を視聴することを強くお勧めします。\n",
    "\n",
    "<!-- ![CS336](https://github.com/yanwunhao/gonken-lesson-build-llm-from-scratch/blob/main/figs/cs336.png?raw=true) -->\n",
    "\n",
    "## このコースを作った理由は？\n",
    "\n",
    "8年前、研究者は自分でモデルを実装し訓練していた。\n",
    "\n",
    "6年前、研究者はモデル（例：BERT）をダウンロードしてファインチューニングしていた。\n",
    "\n",
    "現在、研究者はプロプライエタリなモデル（例：GPT-4/Claude/Gemini）にプロンプトを投げるだけ。\n",
    "\n",
    "抽象度を上げることで生産性は向上するが、\n",
    "* これらの抽象化には漏れがある（プログラミング言語やOSとは対照的に）。\n",
    "* スタックを解体する必要がある基礎研究がまだ残っている。\n",
    "\n",
    "この技術の**完全な理解**は研究に必要である。\n",
    "\n",
    "しかし、現在、ほとんどの研究機関は非常に現実的な問題に直面しています。それが**言語モデルの産業化**です。\n",
    "\n",
    "* GPT-4は推定1.8兆パラメータを持つとされる。\n",
    "* GPT-4の訓練には推定1億ドルかかったとされる。\n",
    "* xAIはGrokを訓練するために20万台のH100でクラスターを構築。\n",
    "* Stargate（OpenAI、NVIDIA、Oracle）は4年間で5000億ドルを投資\n",
    "\n",
    "![stargate](https://github.com/yanwunhao/gonken-lesson-build-llm-from-scratch/blob/main/figs/sb_stargate.png?raw=true)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9237e45",
   "metadata": {},
   "source": [
    "また、フロンティアモデルがどのように構築されているかについて、公開されている詳細は存在しない。\n",
    "\n",
    "GPT-4技術レポートより [Achiam, Josh, et al. \"Gpt-4 technical report.\" arXiv preprint arXiv:2303.08774 (2023).](https://arxiv.org/abs/2303.08774)\n",
    "\n",
    "**“競争環境と、GPT-4のような大規模モデルの安全性への影響の両方を考慮し、本レポートにはアーキテクチャ（モデルサイズを含む）、ハードウェア、訓練計算量、データセット構築、訓練方法、またはその他類似の詳細については一切含まれていない”**\n",
    "\n",
    "![gpt_no_detial](https://github.com/yanwunhao/gonken-lesson-build-llm-from-scratch/blob/main/figs/gpt4-no-details.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e465e6",
   "metadata": {},
   "source": [
    "## Section 1 More is different\n",
    "\n",
    "フロンティアモデルは学校の研究者たちの手の届かないところにある。\n",
    "\n",
    "しかし、小規模言語モデルを構築することは、大規模言語モデルを代表するものではないかもしれない。\n",
    "\n",
    "Example 1: アテンション対MLPで費やされるFLOPsの割合はスケールとともに変化する。\n",
    "\n",
    "![roller](https://github.com/yanwunhao/gonken-lesson-build-llm-from-scratch/blob/main/figs/roller-flops.png?raw=true)\n",
    "\n",
    "Example 2: スケールに伴う振る舞いの創発\n",
    "\n",
    "[Wei, Jason, et al. \"Emergent abilities of large language models.\" arXiv preprint arXiv:2206.07682 (2022).](https://arxiv.org/abs/2206.07682)\n",
    "\n",
    "![wei](https://github.com/yanwunhao/gonken-lesson-build-llm-from-scratch/blob/main/figs/wei-emergence-plot.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a89a890",
   "metadata": {},
   "source": [
    "## Section 2 学校で学べることのうち、産業界のモデルに転用できるものは何か？\n",
    "\n",
    "知識には3つのタイプがある：\n",
    "\n",
    "* メカニクス（仕組み）：物事がどう機能するか（Transformerとは何か、モデル並列化がGPUをどう活用するか）\n",
    "* マインドセット（思考法）：ハードウェアから最大限を引き出すこと、スケールを真剣に捉えること（スケーリング則）\n",
    "* 直感：どのデータとモデリングの決定が良い精度をもたらすか\n",
    "\n",
    "メカニクスとマインドセットは転用可能\n",
    "\n",
    "直感は部分的にしか学校で教えられない（スケール間で必ずしも転用できるとは限らない）。\n",
    "\n",
    "直感とは？\n",
    "\n",
    "一部の設計判断は単に（まだ）正当化できず、実験から得られたものに過ぎない。\n",
    "\n",
    "Example: SwiGLU （From Google）\n",
    "\n",
    "[Shazeer, Noam. \"Glu variants improve transformer.\" arXiv preprint arXiv:2002.05202 (2020).](https://arxiv.org/abs/2002.05202)\n",
    "\n",
    "**”私たちは、これらのアーキテクチャがなぜ機能するように見えるのかについて、何の説明も提供しない。その成功は、他のすべてと同様に、神の慈悲によるものだと考えている。”**\n",
    "\n",
    "![SwiGLU](https://github.com/yanwunhao/gonken-lesson-build-llm-from-scratch/blob/main/figs/divine-benevolence.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b31b14f",
   "metadata": {},
   "source": [
    "## Section 3 LLMにとって効率は一番大事なこと\n",
    "\n",
    "精度 = 効率 × 計算リソース\n",
    "\n",
    "実際、効率性は”大規模”言語モデルになるほど遥かに重要である（無駄にする余裕はない）。\n",
    "\n",
    "Hernandez, Danny, and Tom B. Brown. \"Measuring the algorithmic efficiency of neural networks.\" arXiv preprint arXiv:2005.04305 (2020). (From OpenAI)\n",
    "\n",
    "2012年から2019年の間にImageNetで44倍のアルゴリズム効率を示した。\n",
    "\n",
    "フレーミング：特定の計算量とデータ予算で構築できる最良のモデルは何か？\n",
    "\n",
    "言い換えれば、効率を最大化する！\n",
    "\n",
    "[Liu, Aixin, et al. \"Deepseek-v3 technical report.\" arXiv preprint arXiv:2412.19437 (2024).](https://arxiv.org/abs/2412.19437)\n",
    "\n",
    "DeepSeek-V3の訓練エポック\n",
    "\n",
    "**事前訓練段階（Pre-training）**\n",
    "\n",
    "訓練データ量は14.8兆トークン\n",
    "具体的なエポック数は明記されていないが、データ量から見て1エポック未満の可能性が高い。\n",
    "\n",
    "**教師ありFine-tuning段階（SFT）**\n",
    "\n",
    "明確に2エポック訓練\n",
    "コサイン減衰学習率を使用、5×10⁻⁶から1×10⁻⁶に減少。\n",
    "\n",
    "訓練コスト\n",
    "DeepSeek-V3の完全な訓練には278.8万H800 GPU時間のみ必要で、1時間あたり2ドルで計算すると、総コストは約558万ドル。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d056ad4",
   "metadata": {},
   "source": [
    "## Question：1024台のH100で70Bパラメータモデルを15Tトークンで訓練するのにどれくらい時間がかかるか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2936e339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143.92659381842682\n"
     ]
    }
   ],
   "source": [
    "total_flops = 6 * 70e9 * 15e12 # @inspect total_flops\n",
    "h100_flop_per_sec = 1979e12 / 2\n",
    "mfu = 0.5\n",
    "flops_per_day = h100_flop_per_sec * mfu * 1024 * 60 * 60 * 24 # @inspect flops_per_day\n",
    "days = total_flops / flops_per_day # @inspect days\n",
    "\n",
    "print(days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e2f826",
   "metadata": {},
   "source": [
    "## Section 4 Scaling laws\n",
    "\n",
    "問題：FLOPs予算が与えられたとき、より大きなモデル(D)を使用するか、より多くのトークン(N)で訓練するか？\n",
    "\n",
    "計算最適Scaling laws: \n",
    "\n",
    "[Kaplan, Jared, et al. \"Scaling laws for neural language models.\" arXiv preprint arXiv:2001.08361 (2020).](https://arxiv.org/abs/2001.08361)\n",
    "\n",
    "[Hoffmann, Jordan, et al. \"Training compute-optimal large language models.\" arXiv preprint arXiv:2203.15556 (2022).](https://arxiv.org/abs/2203.15556)\n",
    "\n",
    "![scaling_laws](https://github.com/yanwunhao/gonken-lesson-build-llm-from-scratch/blob/main/figs/chinchilla-isoflop.png?raw=true)\n",
    "\n",
    "**D=20N**（例：14億パラメータモデルは280億トークンで訓練すべき）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a780fb",
   "metadata": {},
   "source": [
    "# Section 5 **nanoGPT**: 簡潔で完全なGPT実装\n",
    "\n",
    "Andrej Karpathyが開発したnanoGPTは、教育用に設計されたGPT-2の再現プロジェクトです：\n",
    "* コード量は500行未満だが、完全な訓練パイプラインを含む\n",
    "* 単一GPUで、連続したテキストを生成できるモデルを訓練可能\n",
    "* 「ゼロから」言語モデルを構築する核心原理を体現\n",
    "\n",
    "GitHub: https://github.com/karpathy/nanoGPT\n",
    "\n",
    "**次回以降のレッスン（３－４回）では、nanoGPTをステップバイステップで一緒に実装していきます。Tokenizerの原理やByte Pair Encoding（BPE）、Transformerの各層の実装、訓練ループの構築など、コードを書きながら深く理解していきましょう。**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
