{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1a3606d",
   "metadata": {},
   "source": [
    "# レッスン 01  Predict-Next-Token\n",
    "\n",
    "このチュートリアルでは、言語モデルの基本メカニズムである「次のトークン予測」（Predict-Next-Token）を深く理解していきます。ゼロから文字レベルの言語モデルを構築し、シェイクスピアの文体を学習させます。この実践プロジェクトを通じて、ニューラルネットワークがどのようにテキストパターンを学習し、特定のスタイルのテキストを生成するかを体験できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89132eb3",
   "metadata": {},
   "source": [
    "## Section 1 シェイクスピアテキストデータセット\n",
    "言語モデルの構築を始める前に、まずトレーニングデータを準備する必要があります。ここでは古典的な「Tiny Shakespeare」データセット——シェイクスピアの全作品を含むテキストコレクションを使用します。\n",
    "\n",
    "### なぜこのデータセットを選ぶのか？\n",
    "\n",
    "* 独特なテキストスタイル：シェイクスピアの古英語の文体は特徴的で、モデルの学習効果を観察しやすい\n",
    "* 適度なデータ量：約1MBのテキストで、高速なトレーニングと実験に適している\n",
    "* 古典的な教育事例：言語モデル入門チュートリアルで広く使用されている\n",
    "\n",
    "### データセットのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ac4b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-08 08:32:20--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2025-10-08 08:32:20 (29.3 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a515c3b6",
   "metadata": {},
   "source": [
    "このコマンドでGitHubからinput.txtファイルを現在のディレクトリにダウンロードします。\n",
    "\n",
    "### テキストデータの読み込み\n",
    "\n",
    "このコードは、テキストファイルを読み込み、データセットの規模を確認する方法を示しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51f7712d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715692bd",
   "metadata": {},
   "source": [
    "with open() ステートメント\n",
    "\n",
    "* with はPythonのコンテキストマネージャーで、ファイルを自動的に閉じ、リソースリークを防ぎます\n",
    "* エラーが発生しても、ファイルは正しく閉じられます\n",
    "* \"r\": 読み取りモード（read mode）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94b4f68",
   "metadata": {},
   "source": [
    "### テキストの冒頭を確認\n",
    "テキストの冒頭を確認任意のデータセットを処理する前に、実際の内容をプレビューすることは非常に重要なステップです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4db7782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a6fd6",
   "metadata": {},
   "source": [
    "## Section 2 Pytorchをインストールする\n",
    "\n",
    "PyTorchをインストールする前に、コンピュータのGPU構成を確認する必要があります。ニューラルネットワークのトレーニング時、GPUは計算速度を大幅に向上させます（通常CPUの100倍以上速い）。\n",
    "\n",
    "### ステップ1：GPUとCUDAバージョンの確認\n",
    "\n",
    "ターミナルまたはJupyter Notebookで以下のコマンドを実行します："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28d367ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct  8 08:32:21 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060        On  |   00000000:01:00.0  On |                  N/A |\n",
      "| 30%   40C    P8             N/A /  115W |    1636MiB /   8188MiB |     49%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f80c47",
   "metadata": {},
   "source": [
    "### コマンドの説明\n",
    "\n",
    "nvidia-smi (NVIDIA System Management Interface)\n",
    "\n",
    "* NVIDIAが公式に提供するGPU監視ツール\n",
    "\n",
    "* GPUモデル、ドライババージョン、CUDAバージョン、VRAMの使用状況などの重要な情報を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aeadc7",
   "metadata": {},
   "source": [
    "### 出力例の解説\n",
    "\n",
    "最初の行：ドライバとCUDA情報\n",
    "\n",
    "* NVIDIA-SMI 580.65.06：NVIDIAドライババージョン\n",
    "* Driver Version: 580.65.06：ドライバプログラムバージョン\n",
    "* CUDA Version: 13.0：最重要！ サポートされる最高CUDAバージョン\n",
    "\n",
    "GPU詳細情報（GPU毎に1行）\n",
    "\n",
    "* GPU 0, 1：2枚のGPUが検出されました\n",
    "* NVIDIA H100 80GB HBM3：GPUモデル\n",
    "* 26C：現在の温度26℃\n",
    "* 69W / 700W：現在の消費電力/最大消費電力\n",
    "* 4MiB / 81559MiB：VRAM使用状況 - 使用中4MB / 合計約80GB\n",
    "* 0%：GPU利用率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd7a86b",
   "metadata": {},
   "source": [
    "### ステップ2: PyTorchのインストール\n",
    "\n",
    "PyTorch公式サイトのインストールページにアクセス https://pytorch.org/\n",
    "\n",
    "環境に応じて適切な設定を選択します：\n",
    "\n",
    "![install_pytorch](https://github.com/yanwunhao/gonken-lesson-build-llm-from-scratch/blob/main/figs/install_pytorch.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7cc30d",
   "metadata": {},
   "source": [
    "### インストールの確認\n",
    "\n",
    "インストール完了後、以下のコードを実行して確認："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb9070d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorchバージョン: 2.8.0+cu129\n",
      "CUDAが利用可能: True\n",
      "利用可能なGPU数: 1\n",
      "現在のGPU: NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# PyTorchバージョンの確認\n",
    "print(f\"PyTorchバージョン: {torch.__version__}\")\n",
    "\n",
    "# CUDAが利用可能か確認\n",
    "print(f\"CUDAが利用可能: {torch.cuda.is_available()}\")\n",
    "\n",
    "# CUDAが利用可能な場合、GPU情報を表示\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"利用可能なGPU数: {torch.cuda.device_count()}\")\n",
    "    print(f\"現在のGPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9bfd32",
   "metadata": {},
   "source": [
    "## Section 3 Predict-Next-Token\n",
    "\n",
    "Predict-Next-Token（次のトークン予測） は、すべての現代的な大規模言語モデル（GPT、Claudeなど）の基礎メカニズムです。この概念を理解することがLLMをマスターする鍵となります。\n",
    "\n",
    "LLM(例えばGPT)が実際に行っていることは？GPTの本質は最適な次の文字を一文字ずつ探索することです。具体例で理解しましょう：\n",
    "\n",
    "![LLM1](https://github.com/yanwunhao/gonken-lesson-build-llm-from-scratch/blob/main/figs/llm1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51475ee1",
   "metadata": {},
   "source": [
    "例：「富士山」を生成する\n",
    "\n",
    "ステップ1：\n",
    "入力：\"日本の一番有名な山は？\"\n",
    "GPTモデル予測 → 出力：\"富\"\n",
    "\n",
    "ステップ2：\n",
    "入力：\"日本の一番有名な山は？富\"\n",
    "GPTモデル予測 → 出力：\"士\"\n",
    "\n",
    "ステップ3：\n",
    "入力：\"日本の一番有名な山は？富士\"\n",
    "GPTモデル予測 → 出力：\"山\"\n",
    "\n",
    "ステップ4：\n",
    "入力：\"日本の一番有名な山は富士山です。\"\n",
    "GPTモデル予測 → 出力：\"(END)\" （生成終了）\n",
    "\n",
    "このプロセスを自己回帰生成（Autoregressive Generation）と呼びます：毎回一文字を予測し、その予測結果を入力に追加して、次を予測し続けます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22edc24",
   "metadata": {},
   "source": [
    "### 確率分布：モデルの「思考」プロセス\n",
    "\n",
    "LLM(GPT)は単純に一文字を出力するのではなく、可能なすべての文字に対して確率を計算します：\n",
    "\n",
    "![LLM2](https://github.com/yanwunhao/gonken-lesson-build-llm-from-scratch/blob/main/figs/llm2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ba28b",
   "metadata": {},
   "source": [
    "重要なポイント：\n",
    "* モデルはすべての可能な文字の確率を計算\n",
    "* 最も高い確率の文字が必ずしも選ばれるわけではない（これがAIに「創造性」がある理由）\n",
    "* 温度パラメータで選択のランダム性を制御可能\n",
    "\n",
    "### 「嘘つきの可能性があり」\n",
    "\n",
    "* モデルは統計的なパターンに基づいて次の文字を予測するだけ\n",
    "* 確率が低いオプションも選ばれる可能性がある\n",
    "* モデルは事実を「理解」せず、トレーニングデータのパターンを模倣するだけ\n",
    "\n",
    "入力：\"日本の一番有名な山は？富\"\n",
    "\n",
    "可能な出力：\n",
    "✓ \"士山\" (正しい、確率60%)\n",
    "✗ \"山県\" (間違い、でも確率3%で選ばれる可能性あり)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb97df6",
   "metadata": {},
   "source": [
    "## Section 4 シンプルなシェイクスピアLLMの実装する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc328688",
   "metadata": {},
   "source": [
    "### 語彙表（Vocabulary）の構築\n",
    "\n",
    "言語モデルをトレーニングする前に、データにどの文字が含まれているかを知る必要があります。このプロセスを語彙表の構築と呼びます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b304074d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447fe30",
   "metadata": {},
   "source": [
    "set() は集合を作成し、重複要素を自動的に削除\n",
    "シェイクスピアテキストの場合、出現したすべての異なる文字を抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f898ee3",
   "metadata": {},
   "source": [
    "### 文字を数値に変換\n",
    "\n",
    "ニューラルネットワークは数値しか処理できないため、文字↔数値の双方向マッピングを構築する必要があります。このコードはエンコーダーとデコーダーを実装しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc45330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# 文字から整数へのマッピングを作成\n",
    "# enumerate()で各文字にインデックス（0, 1, 2...）を割り当てる\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}  # string to integer: 文字→数値の辞書\n",
    "\n",
    "# 整数から文字へのマッピングを作成（stoiの逆マッピング）\n",
    "itos = {i: ch for i, ch in enumerate(chars)}  # integer to string: 数値→文字の辞書\n",
    "\n",
    "# エンコーダー: 文字列を受け取り、整数のリストを出力\n",
    "# 例: \"hi\" → [43, 44]\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "\n",
    "# デコーダー: 整数のリストを受け取り、文字列を出力  \n",
    "# 例: [43, 44] → \"hi\"\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "# エンコードのテスト\n",
    "encoded_hii_there = encode(\"hii there\")\n",
    "print(encoded_hii_there)  # 数値リストが表示される\n",
    "\n",
    "# デコードのテスト（元の文字列に戻る）\n",
    "print(decode(encoded_hii_there))  # \"hii there\" が表示される"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7330705",
   "metadata": {},
   "source": [
    "encodeはLAMBDA式から関数に変換："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "646d1987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    \"\"\"\n",
    "    文字列sを数値リストに変換\n",
    "    引数: s (str) - エンコードする文字列\n",
    "    戻り値: list[int] - 数値のリスト\n",
    "    \"\"\"\n",
    "    result = []  # 結果を格納するリスト\n",
    "    for c in s:  # 文字列の各文字をループ\n",
    "        result.append(stoi[c])  # 文字を数値に変換して追加\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44de1360",
   "metadata": {},
   "source": [
    "### 練習問題1 decode関数を実装してください。ヒント: encodeの逆の処理を行います\n",
    "* 整数のリストを受け取る\n",
    "* 各整数をitosで文字に変換\n",
    "* 最後に文字を結合して文字列にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2afc01fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decode(s):\n",
    "    # pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6278346",
   "metadata": {},
   "source": [
    "### データをPyTorchテンソルに変換\n",
    "\n",
    "エンコード済みのテキストデータをPyTorchが処理できる形式——テンソル（Tensor）に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2151cf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59])\n"
     ]
    }
   ],
   "source": [
    "# データセット全体をエンコードしてtorch.Tensorに格納する\n",
    "import torch  # PyTorchをインポート\n",
    "\n",
    "# テキスト全体を数値に変換し、PyTorchのテンソルに変換\n",
    "# dtype=torch.long は64ビット整数型を指定（文字IDを格納するため）\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# テンソルの形状とデータ型を表示\n",
    "print(data.shape, data.dtype)  # 例: torch.Size([1115394]) torch.int64\n",
    "\n",
    "# 最初の200文字分の数値を表示\n",
    "print(data[:200])  # tensor([18, 47, 56, 57, ...]) のような数値列が表示される"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bae6c9",
   "metadata": {},
   "source": [
    "### テンソル（Tensor）とは？\n",
    "\n",
    "テンソルはPyTorchの中核データ構造で、以下のように理解できます：\n",
    "\n",
    "* 0次元テンソル：スカラー（単一の数値）→ 5\n",
    "* 1次元テンソル：ベクトル（配列）→ [1, 2, 3, 4]\n",
    "* 2次元テンソル：行列 → [[1,2], [3,4]]\n",
    "* 3次元以上：高次元配列\n",
    "\n",
    "我々のデータは1次元テンソルで、111万文字のIDを含みます。\n",
    "\n",
    "### データセット分割：トレーニングセットと検証セット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dce564f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットを訓練用と検証用に分割\n",
    "n = int(0.9 * len(data))  # 最初の90%を訓練用、残り10%を検証用に\n",
    "train_data = data[:n]      # 訓練データ：最初の90%\n",
    "val_data = data[n:]        # 検証データ：残りの10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c120e012",
   "metadata": {},
   "source": [
    "### トレーニングサンプルの作成：コンテキストとターゲット\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b849a2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "# コンテキストウィンドウのサイズを定義\n",
    "block_size = 8  # 一度に見る文字数（コンテキスト長）\n",
    "\n",
    "# 訓練サンプルを作成：入力xと目標yをずらして切り出す\n",
    "x = train_data[:block_size]          # 入力：最初の8文字\n",
    "y = train_data[1 : block_size + 1]   # 目標：1文字ずらした8文字\n",
    "\n",
    "# すべての可能なコンテキスト長でトレーニングサンプルを表示\n",
    "for t in range(block_size):\n",
    "    context = x[: t + 1]  # コンテキスト：長さ1からblock_sizeまで増やす\n",
    "    target = y[t]         # ターゲット：次の文字\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5214ee8c",
   "metadata": {},
   "source": [
    "* block_sizeはモデルが一度に見るコンテキストの文字数を定義\n",
    "* 同じテキストから複数のトレーニングサンプル（長さ1〜block_size）を生成し、訓練効率を向上\n",
    "* 各サンプルは「前の文脈から次の文字を予測」という形式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36d5564",
   "metadata": {},
   "source": [
    "### バッチデータローダー\n",
    "\n",
    "複数のトレーニングサンプルを並列処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dda5d38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "# 再現性のため乱数シードを固定\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "batch_size = 4  # 並列処理するシーケンスの数\n",
    "block_size = 8  # 予測のための最大コンテキスト長\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    訓練用または検証用のミニバッチを生成\n",
    "    引数: split - \"train\" または \"val\"\n",
    "    戻り値: (x, y) - 入力テンソルとターゲットテンソル\n",
    "    \"\"\"\n",
    "    # データセットを選択（訓練用または検証用）\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    \n",
    "    # ランダムな開始位置を生成（batch_size個）\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # 入力x: 各開始位置からblock_size分の文字を取得\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    \n",
    "    # ターゲットy: xより1文字先にずらしたデータ\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\n",
    "# バッチデータを取得\n",
    "xb, yb = get_batch(\"train\")\n",
    "\n",
    "print(\"inputs:\")\n",
    "print(xb.shape)  # 形状: [batch_size, block_size]\n",
    "print(xb)\n",
    "\n",
    "print(\"targets:\")\n",
    "print(yb.shape)  # 形状: [batch_size, block_size]\n",
    "print(yb)\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "# バッチ内のすべてのトレーニングサンプルを表示\n",
    "for b in range(batch_size):      # バッチ次元をループ\n",
    "    for t in range(block_size):  # 時間次元をループ\n",
    "        context = xb[b, : t + 1]  # コンテキスト（長さ1〜block_size）\n",
    "        target = yb[b, t]         # 次の文字\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc7ec13",
   "metadata": {},
   "source": [
    "* batch_size=4：一度に4つの独立したテキストシーケンスを並列処理し、訓練効率を向上\n",
    "* ランダムサンプリング：データセットからランダムに開始位置を選択し、モデルが固定順序を記憶するのを防ぐ\n",
    "* torch.stack：複数の1Dテンソルを2Dテンソル [batch_size, block_size] にスタック\n",
    "* 1つのバッチには batch_size × block_size = 4 × 8 = 32 個のトレーニングサンプルが含まれる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e0e328",
   "metadata": {},
   "source": [
    "### 最もシンプルな言語モデルを構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e41dd1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 再現性のため乱数シードを固定\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Bigramモデル: 直前の1文字だけを見て次の文字を予測する最もシンプルなLLM\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # 各トークンが次のトークンのロジット（確率の元）を直接読み取るルックアップテーブル\n",
    "        # 形状: [vocab_size, vocab_size] - 各文字が次の各文字への「好み」を持つ\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        順伝播: 入力から予測を生成\n",
    "        idx: (B, T) - バッチサイズB、シーケンス長Tの入力インデックス\n",
    "        targets: (B, T) - 正解ラベル（訓練時のみ）\n",
    "        \"\"\"\n",
    "        # idxとtargetsはどちらも (B,T) の整数テンソル\n",
    "        # ルックアップテーブルから各トークンの予測を取得\n",
    "        logits = self.token_embedding_table(idx)  # (B, T, C) - C=vocab_size\n",
    "        \n",
    "        # 損失の計算（訓練時のみ）\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # PyTorchのcross_entropyは2D入力を期待するため形状を変換\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)      # (B*T, C) に平坦化\n",
    "            targets = targets.view(B*T)       # (B*T) に平坦化\n",
    "            # クロスエントロピー損失: 予測と正解の差を測定\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        テキスト生成: 現在のコンテキストから新しいトークンを生成\n",
    "        idx: (B, T) - 現在のコンテキストのインデックス配列\n",
    "        max_new_tokens: 生成する新しいトークンの数\n",
    "        \"\"\"\n",
    "        # max_new_tokens分だけ繰り返す\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 予測を取得\n",
    "            logits, loss = self(idx)\n",
    "            \n",
    "            # 最後のタイムステップのみに注目（Bigramは直前の1文字だけを使用）\n",
    "            logits = logits[:, -1, :]  # (B, C) になる\n",
    "            \n",
    "            # ソフトマックスを適用して確率分布に変換\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            \n",
    "            # 確率分布からサンプリング（ランダムに選択）\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            \n",
    "            # サンプリングしたインデックスを実行中のシーケンスに追加\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "# モデルのインスタンス化\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "\n",
    "# 訓練バッチで順伝播を実行\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)  # 予測の形状を表示\n",
    "print(loss)          # 初期損失を表示（訓練前なのでランダム）\n",
    "\n",
    "# テキスト生成のテスト\n",
    "# torch.zeros((1, 1), dtype=torch.long) は改行文字（インデックス0）から開始\n",
    "print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b014df9c",
   "metadata": {},
   "source": [
    "## モデルのトレーニングとテキスト生成\n",
    "\n",
    "モデルをトレーニングしてシェイクスピア風テキストを生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1717fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.382369041442871\n",
      "\n",
      "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;LUCEO, oraingofof win!\n",
      "RIfans picspeserer hee tha,\n",
      "TOFonk? me ain ckntoty ded. bo'llll st ta d:\n",
      "ELIS me hurf lal y, ma dus pe athouo\n",
      "BEY:! Indy; by s afreanoo adicererupa anse tecorro llaus a!\n",
      "OLeneerithesinthengove fal amas trr\n",
      "TI ar I t, mes, n IUSt my w, fredeeyove\n",
      "THek' merer, dd\n",
      "We ntem lud engitheso; cer ize helorowaginte the?\n",
      "Thak orblyoruldvicee chot, p,\n",
      "Bealivolde Th li\n"
     ]
    }
   ],
   "source": [
    "# PyTorchのオプティマイザー（最適化アルゴリズム）を作成\n",
    "# AdamW: 学習率を自動調整する高性能な最適化手法\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 32  # バッチサイズを32に設定\n",
    "\n",
    "# トレーニングループ: 10000ステップ繰り返す\n",
    "for steps in range(10000):  # より良い結果のためステップ数を増やす\n",
    "\n",
    "    # 訓練データからバッチをサンプリング\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # 損失を評価（予測と正解の差を計算）\n",
    "    logits, loss = m(xb, yb)\n",
    "    \n",
    "    # 勾配をゼロにリセット（前回の勾配を消去）\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # 誤差逆伝播: 損失から勾配を計算\n",
    "    loss.backward()\n",
    "    \n",
    "    # パラメータを更新（勾配降下法）\n",
    "    optimizer.step()\n",
    "\n",
    "# 最終的な損失を表示\n",
    "print(loss.item())\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d5ab60",
   "metadata": {},
   "source": [
    "## 次回の予告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3961021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "0.209729 M parameters\n",
      "step 0: train loss 4.4116, val loss 4.4022\n",
      "step 1000: train loss 2.1005, val loss 2.1338\n",
      "step 2000: train loss 1.8832, val loss 1.9785\n",
      "step 3000: train loss 1.7747, val loss 1.9133\n",
      "step 4000: train loss 1.7249, val loss 1.8786\n",
      "step 4999: train loss 1.6673, val loss 1.8170\n",
      "\n",
      "\n",
      "YROMBERLA:\n",
      "Ock, and is the tombanded boay uservater--\n",
      "\n",
      "MARCILLAUUE:\n",
      "Wart he usque, toubart that ane away, my feand' to zoloug\n",
      "Yourselvefuit here to the will,\n",
      "Which ensend, will is the overs, and if the honourable. Ahland me like us, oncriby: but have the tybunle.\n",
      "\n",
      "SICINIUS:\n",
      "Alay sweep\n",
      "Is would thake only so whrowerings to them,\n",
      "His hands in he poor of his but to dangert,\n",
      "If so;\n",
      "Angies must with aled atters\n",
      "Marry, I home, and some strebled:\n",
      "Shone Was happest hoights knear teyour-busich\n",
      "To for his neever kind my lose and gand me\n",
      "That he see--'\n",
      "\n",
      "NORUS:\n",
      "And madam? whock blive with welcome,\n",
      "Thou counfepy to the might. \n",
      "ELAURET:\n",
      "For injursored and be tooget, if parms\n",
      "Wouch in meedy some so upon sixected have not.\n",
      "\n",
      "GLOUCESTER:\n",
      "He gon is, I do, mooth any the beakes againsting wear soul.\n",
      "\n",
      "CAPLO:\n",
      "Fool the welconestrive, must honour;\n",
      "Now it he poiven in the eatures, the touthius\n",
      "To ornouncee suble wip.\n",
      "\n",
      "GLOUCEMBERLA:\n",
      "You bruiles, Begoly manock of an the bodder\n",
      "Beconeive wout it for Glood conforece roce,\n",
      "And for alliveren Henrieg. She'll tou how served tanoughns:\n",
      "Way! hose against, mast thinks and enour infidend--\n",
      "And greven a word, and a borth-and brick\n",
      "AInd my all time to make the and inkeepoded me attely true eide,\n",
      "Awamn it what she would in the detious' flandeds to must brook!\n",
      "And for thee ofjeca deem of thrifed here!\n",
      "Hears, be love is stadve. Wouch done's the king munds begred\n",
      "To cavone, the town, must a belew now;\n",
      "I the depeart om Thriek, some thel!\n",
      "Heaves you our son onje, and hide, so sir!\n",
      "\n",
      "BHARD:\n",
      "\n",
      "CLADY ANWADY Chitity but of a-lathce!\n",
      "Countrect, And when servaring strar\n",
      "Ere LoxcEd being have, any patrove,\n",
      "This were desenon rour unather,-Gark.\n",
      "In, that becould him, by assio's god--thear have blay not,\n",
      "There wifl not servant, heave on voice, of munge,\n",
      "Aolly talk weak the swouch afood o' of the burd blive insureliful, 'punint\n",
      "And out and was falk: not fance your yong mond me,\n",
      "I wose tronga. It welcome, not is Pa parforive,\n",
      "Bit shown'd fould his suppection'd beide!\n",
      "\n",
      "\n",
      "MENE\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16  # how many independent sequences will we process in parallel?\n",
    "block_size = 32  # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 1000\n",
    "learning_rate = 1e-3\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "print(\"Running on \" + device)\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [\n",
    "    stoi[c] for c in s\n",
    "]  # encoder: take a string, output a list of integers\n",
    "decode = lambda l: \"\".join(\n",
    "    [itos[i] for i in l]\n",
    ")  # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"one head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B,T,C)\n",
    "        q = self.query(x)  # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,C)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, \"M parameters\")\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(\n",
    "            f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
    "        )\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
