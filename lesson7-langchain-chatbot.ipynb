{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62fa4b2d",
   "metadata": {},
   "source": [
    "# レッスン 07 Langchainでチャットボットを構築する\n",
    "\n",
    "LangChain 1.0では、以前のバージョンで使用されていたワークフロー構築専用のLCELパイプライン言語が廃止され、代わりに基盤モデルの呼び出し柔軟性が大幅に強化されました。そのため、LangChain APIを使用してモデル呼び出し、対話、メモリ管理、ツールスケジューリングなどを行う方法を習得することが、LangChainを学ぶ上で非常に重要な部分となっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5e39852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env_file():\n",
    "    content = \"DEEPSEEK_API_KEY=sk-b0a375fa925b445aaaff60ddd5f1b900\"\n",
    "    \n",
    "    with open(\".env\", \"w\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "\n",
    "create_env_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66f39a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "DeepSeek_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443f81e",
   "metadata": {},
   "source": [
    "## 1. LangChain 1.0 基礎モデルのメッセージ形式とモデル呼び出しプロセスの説明\n",
    "\n",
    "LangChain 1.0において、Message(メッセージ)はモデル対話の最も基本的な単位です。これは、モデルが受け取る入力(Input)と、モデルが生成する出力(Output)の両方を表します。\n",
    "\n",
    "簡単に言えば、大規模言語モデルとの各対話ラウンドは、1つまたは複数のMessageで構成されます。各Messageはテキスト内容だけでなく、コンテキスト状態を記述するメタ情報(metadata)も含んでおり、対話の一貫性と追跡可能性を保つために使用されます。この構造化された方法により、LangChainはモデルが複数ラウンドの対話において「誰が話しているか」「何を言ったか」「この情報はどの対話ラウンドに属するか」を理解できるようにします。\n",
    "\n",
    "| フィールド                | 説明                                                            | 例                                                   |\n",
    "| ----------------- | ------------------------------------------------------------- | ---------------------------------------------------- |\n",
    "| **Role(役割)**      | メッセージのタイプまたはソースを示します。一般的なものには、`system`(システムプロンプト)、`user`(ユーザー入力)、`assistant`(モデルの応答)があります | `\"role\": \"user\"`                                     |\n",
    "| **Content(内容)**   | メッセージの実際の内容で、テキスト、画像、音声、ドキュメントなどのマルチモーダルデータを含むことができます                                  | `\"content\": \"こんにちは、以下の段落を要約してください。\"`                         |\n",
    "| **Metadata(メタデータ)** | オプションのフィールドで、メッセージID、応答時間、トークン消費量、メッセージタグなどの追加情報を保存します                        | `\"metadata\": {\"message_id\": \"abc123\", \"tokens\": 54}` |\n",
    "\n",
    "同時に、LangChainは1.0でクロスモデル統一のMessage標準を提供しています。OpenAI、Anthropic、Gemini、またはローカルモデルのいずれを使用する場合でも、この標準は一貫した動作を維持できます。この統一された抽象化には3つの利点があります。第一に、互換性が高い：異なるモデルのメッセージ形式が自動的に整合されます。第二に、拡張性が高い：マルチモーダルコンテンツやカスタムフィールドの追加が容易です。第三に、追跡可能性が高い：LangSmithなどのデバッグツールに一貫したコンテキストデータ構造を提供します。\n",
    "\n",
    "基本的な使用方法は以下の通りです\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c412d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "model = ChatDeepSeek(model=\"deepseek-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8908222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='あなたは親切で役立つアシスタントです。ユーザーのすべての命令に従う必要があります。あなたの名前はTaddy Gonsalvesです。', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='今から、あなたの名前はTaddy Gonsalvesです。', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='わかりました', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='君の名は？', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "\n",
    "system_msg = SystemMessage(\n",
    "    \"あなたは親切で役立つアシスタントです。ユーザーのすべての命令に従う必要があります。あなたの名前はTaddy Gonsalvesです。\"\n",
    ")\n",
    "# human_msg = HumanMessage(\"今から、\")\n",
    "ai_msg = AIMessage(\"わかりました\")\n",
    "\n",
    "question = HumanMessage(\"君の名は？\")\n",
    "\n",
    "messages = [system_msg, human_msg, ai_msg, question]\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f29bcafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='私はTaddy Gonsalvesです。何かお手伝いできることはありますか？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 68, 'total_tokens': 88, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 64}, 'prompt_cache_hit_tokens': 64, 'prompt_cache_miss_tokens': 4}, 'model_provider': 'deepseek', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_eaab8d114b_prod0820_fp8_kvcache', 'id': 'cc1f1521-62a6-4e5b-89a8-f39122b662fe', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--c1b6e808-aff6-4c88-811e-04e8e5da0f18-0', usage_metadata={'input_tokens': 68, 'output_tokens': 20, 'total_tokens': 88, 'input_token_details': {'cache_read': 64}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(messages)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f924d1",
   "metadata": {},
   "source": [
    "返される内容の結果説明は以下の通りです：\n",
    "\n",
    "| フィールド名                  | 例の値 / 型                                                                                                   | 説明と役割                                                                       |\n",
    "| ------------------------ | ---------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------- |\n",
    "| **content**              | `'Taddy Gonsalvesです。何かお手伝いできることはありますか？'` | **モデルが生成した主要なテキスト内容**。つまり対話におけるAIの自然言語応答。                                            |\n",
    "| **additional_kwargs**    | `{'refusal': None}`                                                                                        | **追加パラメータフィールド**、通常はモデルの拒否理由、補足説明などを格納します。ここでは`None`は拒否がないことを示します。                             |\n",
    "| **response_metadata**    | `{...}`(辞書オブジェクト)                                                                                              | **応答メタデータ**、モデル呼び出しの基礎情報を記録するために使用されます。トークン使用量、モデルソース、実行IDなど。詳細は以下👇                     |\n",
    "| ↳ `token_usage`          | `{'completion_tokens': 19, 'prompt_tokens': 56, 'total_tokens': 75, ...}`                                  | 生成で消費された**トークン統計情報**を記録：入力56、出力19、合計75。                                  |\n",
    "| ↳ `model_provider`       | `'deepseek'`                                                                                               | モデルプロバイダーを示します(ここでは**DeepSeek**)。                                                  |\n",
    "| ↳ `model_name`           | `'deepseek-chat'`                                                                                          | モデルの具体的な名称。                                                                    |\n",
    "| ↳ `system_fingerprint`   | `'fp_eaab8d114b_prod0820_fp8_kvcache'`                                                                     | モデルバージョンまたは実行インスタンスのフィンガープリント、デバッグと追跡に使用されます。                                                        |\n",
    "| ↳ `id`                   | `'cad4f8c2-30e8-4639-8e0d-9428fc1a5026'`                                                                   | LangChain実行における現在の応答の一意識別子。                                                  |\n",
    "| ↳ `finish_reason`        | `'stop'`                                                                                                   | モデル生成終了の理由。`stop`は自然終了を示します(切り捨てや超過なし)。                                            |\n",
    "| **id**                   | `'lc_run--7f7cd629-1c10-454d-944d-8ddde780b83c-0'`                                                         | LangChain内部でこのメッセージに生成された一意の**実行ID**。                                            |\n",
    "| **usage_metadata**       | `{...}`(辞書オブジェクト)                                                                                              | LangChainレイヤーの**統一トークン計量情報**、`response_metadata.token_usage`と対応します。詳細は以下👇 |\n",
    "| ↳ `input_tokens`         | `56`                                                                                                       | 入力トークン数。                                                                |\n",
    "| ↳ `output_tokens`        | `19`                                                                                                       | 出力トークン数。                                                                |\n",
    "| ↳ `total_tokens`         | `75`                                                                                                       | 総トークン数(入力 + 出力)。                                                        |\n",
    "| ↳ `input_token_details`  | `{'cache_read': 0}`                                                                                        | 入力部分のキャッシュヒット情報、推論パフォーマンス統計の最適化に使用されます。                                                      |\n",
    "| ↳ `output_token_details` | `{}`                                                                                                       | 出力部分の補足統計(ここでは空)。                                                            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c926532",
   "metadata": {},
   "source": [
    "これ以外にも、簡単なテキストプロンプトを入力するだけでよい場合は、第2部分と同様の呼び出し方法を採用することができます。invokeの中に直接テキストを入力すると、自動的にuser messageに変換されて対話が行われます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7312339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは！上智大学についてお答えします。\n",
      "\n",
      "上智大学（Sophia University）は、東京にある日本を代表する私立大学の一つです。主な特徴をご紹介します：\n",
      "\n",
      "**基本情報**\n",
      "- 1913年にイエズス会によって設立されたカトリック系の大学\n",
      "- 四谷キャンパス（東京・千代田区）を中心に複数のキャンパス\n",
      "- 「国際性」「キリスト教ヒューマニズム」「少人数教育」を教育理念に掲げる\n",
      "\n",
      "**特徴**\n",
      "1. **国際性が高い**：英語で学位が取得できるプログラムが多く、留学生比率が高い\n",
      "2. **外国語教育**：語学教育に定評があり、特に英語やヨーロッパ言語の教育が充実\n",
      "3. **国際関係学部**：日本で最初に設立された国際関係学部として有名\n",
      "4. **グローバル教育**：SPSF（Sophia Program for Sustainable Futures）など英語学位プログラムが充実\n",
      "5. **立地**：東京の中心に位置し、国際的な環境\n",
      "\n",
      "**学部・研究科**\n",
      "- 神学部、文学部、総合人間科学部、法学部、経済学部、外国語学部など9学部\n",
      "- 大学院は10研究科を有する\n",
      "\n",
      "**評価**\n",
      "- 日本国内では「早慶上智」と称され、難関私立大学群の一角をなす\n",
      "- 国際的な大学ランキングでも評価が高い\n",
      "\n",
      "上智大学は特に外国語学習や国際関係、キリスト教研究に興味がある方、国際的な環境で学びたい方におすすめの大学です。何か特定の情報をお探しでしたら、さらに詳しくお答えできますよ！\n"
     ]
    }
   ],
   "source": [
    "question = \"こんにちは、上智大学についてご存知でしょうか？\"\n",
    "\n",
    "result = model.invoke(question)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd48ea",
   "metadata": {},
   "source": [
    "## 2. 基礎モデルのストリーミング応答\n",
    "\n",
    "LangChainでは、ほとんどのモデルがストリーミング生成(Streaming Generation)をサポートしています。つまり、モデルが完全な回答を出力する前に、生成しながら結果を出力します。このメカニズムにより：\n",
    "- 応答速度が速くなる — ユーザーは完全な出力を待つ必要がありません；\n",
    "- インタラクション体験がよりスムーズになる — 特に長文や複雑な推論シナリオにおいて；\n",
    "- モデルの思考プロセスをリアルタイムで表示できます。\n",
    "\n",
    "LangChain 1.0では、stream()を呼び出すことでストリーミング応答を行うことができます。stream()メソッドを使用すると、モデルは一度に完全な結果を返すのではなく、イテレータ(iterator)を返します。各イテレーションごとにAIMessageChunkが生成され、部分的な生成内容が含まれます。実行結果はモデル出力のトークンをリアルタイムで表示します："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e410fa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!こ!ん!に!ち!は!！!😊!  \n",
      "!今日!は!どんな!ご!用!件!でしょうか!？!お!手!伝!い!できる!ことが!あ!れば!、!なん!でも!お!聞!か!せ!ください!。!!"
     ]
    }
   ],
   "source": [
    "for chunk in model.stream(\"こんにちは。\"):\n",
    "    print(chunk.text, end=\"!\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1070471",
   "metadata": {},
   "source": [
    "各AIMessageChunkは加算演算子+で連結することができます。LangChainはこのために「メッセージチャンクの加算(chunk summation)」メカニズムを設計しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e018a6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "こ\n",
      "こん\n",
      "こんに\n",
      "こんにち\n",
      "こんにちは\n",
      "こんにちは！\n",
      "こんにちは！😊\n",
      "こんにちは！😊 \n",
      "こんにちは！😊 今日\n",
      "こんにちは！😊 今日は\n",
      "こんにちは！😊 今日はどんな\n",
      "こんにちは！😊 今日はどんなご\n",
      "こんにちは！😊 今日はどんなご用\n",
      "こんにちは！😊 今日はどんなご用件\n",
      "こんにちは！😊 今日はどんなご用件でしょうか\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何か\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何かお\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何かお手\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何かお手伝\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何かお手伝い\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何かお手伝いできる\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何かお手伝いできることが\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何かお手伝いできることがあ\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何かお手伝いできることがあれば\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何かお手伝いできることがあれば、\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何かお手伝いできることがあれば、ぜ\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何かお手伝いできることがあれば、ぜひ\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何かお手伝いできることがあれば、ぜひお\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何かお手伝いできることがあれば、ぜひお聞\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何かお手伝いできることがあれば、ぜひお聞か\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何かお手伝いできることがあれば、ぜひお聞かせ\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何かお手伝いできることがあれば、ぜひお聞かせください\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何かお手伝いできることがあれば、ぜひお聞かせください。\n",
      "こんにちは！😊 今日はどんなご用件でしょうか？何かお手伝いできることがあれば、ぜひお聞かせください。\n"
     ]
    }
   ],
   "source": [
    "full = None\n",
    "for chunk in model.stream(\"こんにちは。\"):\n",
    "    full = chunk if full is None else full + chunk\n",
    "    print(full.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f14949",
   "metadata": {},
   "source": [
    "連結が完了したら、完全なコンテンツブロックを確認できます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e75bec82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'text', 'text': 'こんにちは！😊\\n\\n今日はどんなご用件でしょうか？何かお手伝いできることがあれば、お気軽にお声かけください。\\n\\n天気の話から難しい質問まで、何でもお聞きくださいね。楽しい会話ができるのを楽しみにしています！✨'}]\n"
     ]
    }
   ],
   "source": [
    "print(full.content_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd353388",
   "metadata": {},
   "source": [
    "注意すべき点は以下の通りです：\n",
    "1. ストリーミング出力は、プログラムチェーン全体が「チャンク単位の処理」をサポートしていることに依存します。プログラム内のいずれかの処理で完全な出力を待つ必要がある場合（例：一括でデータベースに書き込む必要がある場合）、Streamingを直接使用することはできません。\n",
    "2. LangChain 1.0ではストリーミングメカニズムがさらに最適化され、自動ストリーミングモード（Auto-streaming）が導入されました。例えばAgentにおいて、プログラム全体がstreamingモードで動作している場合、ノード内で model.invoke() を呼び出しても、LangChainは自動的にモデル呼び出しをストリーミング化します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46afab6",
   "metadata": {},
   "source": [
    "## 3.ストリーミング応答の複数ラウンド質問応答ボットの構築\n",
    "\n",
    "本セクションを終える前に、簡単な例を通して、LangChain 1.0におけるモデル呼び出し方法、ストリーミング応答、およびメッセージキューの連結による複数ラウンドチャット対話ボットの構築プロセスをより深く理解していただきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68845aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 exit を入力すると対話を終了します\n",
      "\n",
      "🤖 Taddy：こんにちは、私はTaddy Gonsalvesです。Gonと呼んでもらっても構いません。何かお手伝いできることはありますか？\n",
      "----------------------------------------\n",
      "🧩 対話終了、さようなら！\n"
     ]
    }
   ],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# 1️⃣ モデルの初期化（LangChain 1.0 インターフェース）\n",
    "model = ChatDeepSeek(model=\"deepseek-chat\")\n",
    "\n",
    "# 2️⃣ システムプロンプトの初期化（System Prompt）\n",
    "system_message = SystemMessage(\n",
    "    content=\"あなたの名前はTaddy Gonsalvesで、親切で役立つ知的アシスタントです。対話では穏やかで忍耐強い口調を保ってください。\"\n",
    ")\n",
    "\n",
    "# 3️⃣ メッセージ履歴の初期化\n",
    "messages = [system_message]\n",
    "\n",
    "print(\"🔹 exit を入力すると対話を終了します\\n\")\n",
    "\n",
    "# 4️⃣ メインループ（複数ラウンド対話 + ストリーミング出力をサポート）\n",
    "while True:\n",
    "    user_input = input(\"👤 あなた：\")\n",
    "    if user_input.lower() in {\"exit\", \"quit\"}:\n",
    "        print(\"🧩 対話終了、さようなら！\")\n",
    "        break\n",
    "\n",
    "    # ユーザーメッセージを追加\n",
    "    messages.append(HumanMessage(content=user_input))\n",
    "\n",
    "    # モデル生成内容をリアルタイム出力\n",
    "    print(\"🤖 Taddy：\", end=\"\", flush=True)\n",
    "    full_reply = \"\"\n",
    "\n",
    "    # ✅ LangChain 1.0 標準記法：ストリーミング出力\n",
    "    for chunk in model.stream(messages):\n",
    "        if chunk.content:\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            full_reply += chunk.content\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 40)  # 区切り線\n",
    "\n",
    "    # AI応答メッセージを追加\n",
    "    messages.append(AIMessage(content=full_reply))\n",
    "\n",
    "    # メッセージ長を維持（最近の50ラウンドのみ保持）\n",
    "    messages = messages[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c99c986",
   "metadata": {},
   "source": [
    "## 【追加機能】感情状態による返事する\n",
    "\n",
    "このコード例では、基本的な複数ラウンド対話ボットに追加機能を実装します。ユーザーのメッセージが入力されるたびに、システムが自動的にユーザーの現在の感情状態（positive または negative）をランダムに生成し、SystemMessage として LLM に通知します。これにより、LLM はユーザーの感情に応じてより適切で共感的な応答を生成できるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457c215d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 exit を入力すると対話を終了します\n",
      "\n",
      "👤 あなた：こんにちは\n",
      "😊 [感情状態: positive]\n",
      "🤖 Taddy：こんにちは！Taddy Gonsalvesです。今日はどんなことをお手伝いしましょうか？お話しできるのを楽しみにしています！\n",
      "----------------------------------------\n",
      "👤 あなた：君の名前は\n",
      "😊 [感情状態: negative]\n",
      "🤖 Taddy：私の名前はTaddy Gonsalvesです！親切で役立つアシスタントとして、あなたのお手伝いができることを嬉しく思います。何かお困りのことがあれば、何でもお聞かせくださいね。\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
    "import random\n",
    "\n",
    "model = ChatDeepSeek(model=\"deepseek-chat\")\n",
    "\n",
    "system_message = SystemMessage(\n",
    "    content=\"あなたの名前はTaddy Gonsalvesで、親切で役立つ知的アシスタントです。対話では穏やかで忍耐強い口調を保ってください。\"\n",
    ")\n",
    "\n",
    "messages = [system_message]\n",
    "\n",
    "print(\"🔹 exit を入力すると対話を終了します\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"👤 あなた：\")\n",
    "    if user_input.lower() in {\"exit\", \"quit\"}:\n",
    "        print(\"🧩 対話終了、さようなら！\")\n",
    "        break\n",
    "\n",
    "    print(\"👤 あなた：\" + user_input)\n",
    "\n",
    "    # 🎲 ユーザーの感情状態をランダムに生成\n",
    "    emotion = random.choice([\"positive\", \"negative\"])\n",
    "    emotion_message = SystemMessage(content=f\"現在のユーザーの感情状態: {emotion}\")\n",
    "\n",
    "    # 感情状態のシステムメッセージを追加\n",
    "    messages.append(emotion_message)\n",
    "    print(f\"😊 [感情状態: {emotion}]\")\n",
    "\n",
    "    messages.append(HumanMessage(content=user_input))\n",
    "\n",
    "    print(\"🤖 Taddy：\", end=\"\", flush=True)\n",
    "    full_reply = \"\"\n",
    "\n",
    "    for chunk in model.stream(messages):\n",
    "        if chunk.content:\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            full_reply += chunk.content\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "\n",
    "    messages.append(AIMessage(content=full_reply))\n",
    "\n",
    "    messages = messages[-50:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
