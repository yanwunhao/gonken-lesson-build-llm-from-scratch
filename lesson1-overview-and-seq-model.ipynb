{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2319c26f",
   "metadata": {},
   "source": [
    "# レッスン 01  LLMの研究概要とSeqモデル\n",
    "\n",
    "このコースは、スタンフォード大学の公開講座であるCS336とCS224N、そしてAndrej KarpathyのYouTubeブログのコンテンツをまとめたものです。\n",
    "お時間があれば、YouTubeでオリジナル動画、**特にCS336**の動画を視聴することを強くお勧めします。\n",
    "\n",
    "<!-- ![CS336](https://github.com/yanwunhao/gonken-lesson-build-llm-from-scratch/blob/main/figs/cs336.png?raw=true) -->\n",
    "\n",
    "## このコースを作った理由は？\n",
    "\n",
    "8年前、研究者は自分でモデルを実装し訓練していた。\n",
    "\n",
    "6年前、研究者はモデル（例：BERT）をダウンロードしてファインチューニングしていた。\n",
    "\n",
    "現在、研究者はプロプライエタリなモデル（例：GPT-4/Claude/Gemini）にプロンプトを投げるだけ。\n",
    "\n",
    "抽象度を上げることで生産性は向上するが、\n",
    "* これらの抽象化には漏れがある（プログラミング言語やOSとは対照的に）。\n",
    "* スタックを解体する必要がある基礎研究がまだ残っている。\n",
    "\n",
    "この技術の**完全な理解**は研究に必要である。\n",
    "\n",
    "しかし、現在、ほとんどの研究機関は非常に現実的な問題に直面しています。それが**言語モデルの産業化**です。\n",
    "\n",
    "* GPT-4は推定1.8兆パラメータを持つとされる。\n",
    "* GPT-4の訓練には推定1億ドルかかったとされる。\n",
    "* xAIはGrokを訓練するために20万台のH100でクラスターを構築。\n",
    "* Stargate（OpenAI、NVIDIA、Oracle）は4年間で5000億ドルを投資\n",
    "\n",
    "![stargate](https://github.com/yanwunhao/gonken-lesson-build-llm-from-scratch/blob/main/figs/sb_stargate.png?raw=true)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
